Homework 3
================
Connie Zhang
10/09/2019

# Problem 1

``` r
instacart = 
  library(p8105.datasets)
  data("instacart")
```

  - The instacart dataset has 1384617 observations, with 15 variables.
    Key variables include order details, product name, aisle, and
    department with respective numeric IDs associated with them. For
    example, the first order
is

| order\_id | product\_id | add\_to\_cart\_order | reordered | user\_id | eval\_set | order\_number | order\_dow | order\_hour\_of\_day | days\_since\_prior\_order | product\_name    | aisle\_id | department\_id | aisle  | department |
| --------: | ----------: | -------------------: | --------: | -------: | :-------- | ------------: | ---------: | -------------------: | ------------------------: | :--------------- | --------: | -------------: | :----- | :--------- |
|         1 |       49302 |                    1 |         1 |   112108 | train     |             4 |          4 |                   10 |                         9 | Bulgarian Yogurt |       120 |             16 | yogurt | dairy eggs |

It is an order of bulgarian yogurt, by user id 112108, placed on 10am on
4th day of week, Wednesday. The yogurt is in aisle 120, in the dairy
eggs department. He purchased this product 4 times before and his last
order was 9 days ago.

Number of aisles and which aisles the most items are ordered from:

``` r
instacart %>%
  count(aisle_id, name = "n") %>% 
arrange (desc(n))
```

    ## # A tibble: 134 x 2
    ##    aisle_id      n
    ##       <int>  <int>
    ##  1       83 150609
    ##  2       24 150473
    ##  3      123  78493
    ##  4      120  55240
    ##  5       21  41699
    ##  6      115  36617
    ##  7       84  32644
    ##  8      107  31269
    ##  9       91  26240
    ## 10      112  23635
    ## # … with 124 more rows

Plot that shows the number of items ordered in each aisle, limited to
aisles with more than 10000 items ordered:

``` r
instacart %>%
  group_by(aisle) %>%
  summarize(aisle_n = n()) %>%
  filter(aisle_n >10000) %>%
  arrange (desc(aisle_n)) %>%
  ggplot(aes(x = aisle, y = aisle_n, color = aisle)) + ggtitle("Quantity of Items Ordered in Each Aisle") +
  theme(plot.title = element_text(hjust = 0.5))+ geom_point() + 
  labs(       x = "aisle",
              y = "number of items ordered in aisle") + viridis::scale_color_viridis(discrete = TRUE) + theme(legend.position = "none", axis.text.x = element_text(angle=70, hjust=1))
```

![](p8105_hw3_cz2540_files/figure-gfm/unnamed-chunk-3-1.png)<!-- -->

Table that shows three most popular items in the aisles “baking
ingredients”, “dog food care”, and “packaged vegetables fruits”. Include
the number of times each item is ordered in your table:

``` r
table_popular = 
instacart %>%
  filter(aisle %in% c("baking ingredients", "dog food care","packaged vegetables fruits")) %>%
  group_by(aisle, product_name) %>%
  summarize(n = n()) %>%
top_n(3) %>%
arrange(desc(n)) %>%
knitr::kable()
```

    ## Selecting by n

``` r
table_popular
```

| aisle                      | product\_name                                 |    n |
| :------------------------- | :-------------------------------------------- | ---: |
| packaged vegetables fruits | Organic Baby Spinach                          | 9784 |
| packaged vegetables fruits | Organic Raspberries                           | 5546 |
| packaged vegetables fruits | Organic Blueberries                           | 4966 |
| baking ingredients         | Light Brown Sugar                             |  499 |
| baking ingredients         | Pure Baking Soda                              |  387 |
| baking ingredients         | Cane Sugar                                    |  336 |
| dog food care              | Snack Sticks Chicken & Rice Recipe Dog Treats |   30 |
| dog food care              | Organix Chicken & Brown Rice Recipe           |   28 |
| dog food care              | Small Dog Biscuits                            |   26 |

Table that shows the mean hour of the day at which Pink Lady Apples and
Coffee Ice Cream are ordered on each day of the week; format this table
for human readers (i.e. produce a 2 x 7 table):

``` r
table_icecream_apple = instacart %>%
  select(product_name, order_dow, order_hour_of_day) %>%
  group_by(product_name, order_dow) %>%
  summarize(mean_hour = mean(order_hour_of_day)) %>%
  mutate(order_dow = recode(order_dow, "0" = "Sunday", "1" = "Monday", "2" = "Tuesday", "3" = "Wednesday", "4" = "Thursday", "5" = "Friday", "6" = "Saturday")) %>%
  filter(product_name %in% c("Pink Lady Apples", "Coffee Ice Cream")) %>%
  pivot_wider(names_from = "order_dow", values_from = "mean_hour") %>%
knitr::kable(digits = 2)
```

# Problem 2

``` r
data("brfss_smart2010") #load data
```

``` r
cleanbrfss_data = 
  brfss_smart2010 %>% #clean data 
  janitor::clean_names() %>% 
  separate(locationdesc, into = c("state", "county"), sep=3) %>%
  select (-locationabbr) %>%
   filter(topic == "Overall Health", response %in% c("Poor","Fair","Good","Very good", "Excellent")) %>% 
  mutate(county = stringr::str_replace(county, "- ", ""), response = factor(response, levels = c("Poor", "Fair","Good", "Very good", "Excellent"))) 
cleanbrfss_data
```

    ## # A tibble: 10,625 x 23
    ##     year state county class topic question response sample_size data_value
    ##    <int> <chr> <chr>  <chr> <chr> <chr>    <fct>          <int>      <dbl>
    ##  1  2010 "AL " Jeffe… Heal… Over… How is … Excelle…          94       18.9
    ##  2  2010 "AL " Jeffe… Heal… Over… How is … Very go…         148       30  
    ##  3  2010 "AL " Jeffe… Heal… Over… How is … Good             208       33.1
    ##  4  2010 "AL " Jeffe… Heal… Over… How is … Fair             107       12.5
    ##  5  2010 "AL " Jeffe… Heal… Over… How is … Poor              45        5.5
    ##  6  2010 "AL " Mobil… Heal… Over… How is … Excelle…          91       15.6
    ##  7  2010 "AL " Mobil… Heal… Over… How is … Very go…         177       31.3
    ##  8  2010 "AL " Mobil… Heal… Over… How is … Good             224       31.2
    ##  9  2010 "AL " Mobil… Heal… Over… How is … Fair             120       15.5
    ## 10  2010 "AL " Mobil… Heal… Over… How is … Poor              66        6.4
    ## # … with 10,615 more rows, and 14 more variables:
    ## #   confidence_limit_low <dbl>, confidence_limit_high <dbl>,
    ## #   display_order <int>, data_value_unit <chr>, data_value_type <chr>,
    ## #   data_value_footnote_symbol <chr>, data_value_footnote <chr>,
    ## #   data_source <chr>, class_id <chr>, topic_id <chr>, location_id <chr>,
    ## #   question_id <chr>, respid <chr>, geo_location <chr>

States that were observed in 2002 at 7 or more locations:

``` r
cleanbrfss_data %>% 
  filter(year == 2002) %>% 
  group_by(state) %>% 
  summarize(n_location = n_distinct(county)) %>% 
  filter(n_location >= 7) %>%
  arrange(n_location)
```

    ## # A tibble: 6 x 2
    ##   state n_location
    ##   <chr>      <int>
    ## 1 "CT "          7
    ## 2 "FL "          7
    ## 3 "NC "          7
    ## 4 "MA "          8
    ## 5 "NJ "          8
    ## 6 "PA "         10

  - In 2002, Connecticut, Florida, Massachusetts, North Carolina, New
    Jersey, and Pennsylvania were observed at 7 or more locations.

States that were observed in 2010 at 7 or more locations:

``` r
cleanbrfss_data %>% 
  filter(year == 2010) %>% 
  group_by(state) %>% 
  summarize(n_location = n_distinct(county)) %>% 
  filter(n_location >= 7) %>%
  arrange(n_location)
```

    ## # A tibble: 14 x 2
    ##    state n_location
    ##    <chr>      <int>
    ##  1 "CO "          7
    ##  2 "PA "          7
    ##  3 "SC "          7
    ##  4 "OH "          8
    ##  5 "MA "          9
    ##  6 "NY "          9
    ##  7 "NE "         10
    ##  8 "WA "         10
    ##  9 "CA "         12
    ## 10 "MD "         12
    ## 11 "NC "         12
    ## 12 "TX "         16
    ## 13 "NJ "         19
    ## 14 "FL "         41

  - In 2010, Colorado, Pennsylvania, South Carolina, Ohio, Maryland, New
    York, Nebraska, Washington, California, Maryland, North Carolina,
    Texas, New Jersey, and Florida were observed in 7 or more locations.

Dataset limited to Excellent responses, and contains, year, state,
averages of the data\_value across locations within a state:

``` r
brfss_excellent = cleanbrfss_data %>% 
  filter(response == "Excellent") %>%
  group_by(year, state, county) %>% 
  summarize(avg_data = mean(data_value))
brfss_excellent
```

    ## # A tibble: 2,125 x 4
    ## # Groups:   year, state [443]
    ##     year state county                 avg_data
    ##    <int> <chr> <chr>                     <dbl>
    ##  1  2002 "AK " Anchorage Municipality     27.9
    ##  2  2002 "AL " Jefferson County           18.5
    ##  3  2002 "AR " Pulaski County             24.1
    ##  4  2002 "AZ " Maricopa County            21.6
    ##  5  2002 "AZ " Pima County                26.6
    ##  6  2002 "CA " Los Angeles County         22.7
    ##  7  2002 "CO " Adams County               21.2
    ##  8  2002 "CO " Arapahoe County            25.5
    ##  9  2002 "CO " Denver County              22.2
    ## 10  2002 "CO " Jefferson County           23.4
    ## # … with 2,115 more rows

“Spaghetti” plot of this average value over time within a state (that
is, make a plot showing a line for each state across years):

``` r
brfss_excellent %>%
ggplot(aes(x = year , y = avg_data, color=state)) + geom_line() + 
labs(title = "Average Value Over Time Within Each State", x = "Year", y = "Average Value") + 
viridis::scale_color_viridis(discrete = TRUE) + theme_minimal() + theme(plot.title = element_text(hjust = 0.5, size = 14), axis.text.x = element_text(angle=70, hjust=1)) + theme(legend.position = "right", axis.text.x = element_text(angle=70, hjust=1))
```

![](p8105_hw3_cz2540_files/figure-gfm/unnamed-chunk-11-1.png)<!-- -->

Two-panel plot showing, for the years 2006, and 2010, distribution of
data\_value for responses (“Poor” to “Excellent”) among locations in NY
State:

# Problem 3

``` r
acc_data = 
  read_csv("./data/accel_data.csv") %>%
  janitor::clean_names() %>%
  pivot_longer (
    activity_1:activity_1440, names_to = "activity_min", values_to = "activity_count") %>%
  mutate(activity_min = stringr::str_replace(activity_min, "activity_", " ")) %>%
  mutate(weekkday = day %in% c("Monday", "Tuesday", "Wednesday", "Thursday", "Friday"), weekend = day %in% c("Saturday", "Sunday"))
```

    ## Parsed with column specification:
    ## cols(
    ##   .default = col_double(),
    ##   day = col_character()
    ## )

    ## See spec(...) for full column specifications.

``` r
acc_data
```

    ## # A tibble: 50,400 x 7
    ##     week day_id day    activity_min activity_count weekkday weekend
    ##    <dbl>  <dbl> <chr>  <chr>                 <dbl> <lgl>    <lgl>  
    ##  1     1      1 Friday " 1"                   88.4 TRUE     FALSE  
    ##  2     1      1 Friday " 2"                   82.2 TRUE     FALSE  
    ##  3     1      1 Friday " 3"                   64.4 TRUE     FALSE  
    ##  4     1      1 Friday " 4"                   70.0 TRUE     FALSE  
    ##  5     1      1 Friday " 5"                   75.0 TRUE     FALSE  
    ##  6     1      1 Friday " 6"                   66.3 TRUE     FALSE  
    ##  7     1      1 Friday " 7"                   53.8 TRUE     FALSE  
    ##  8     1      1 Friday " 8"                   47.8 TRUE     FALSE  
    ##  9     1      1 Friday " 9"                   55.5 TRUE     FALSE  
    ## 10     1      1 Friday " 10"                  43.0 TRUE     FALSE  
    ## # … with 50,390 more rows

  - This dataset has 50400 observations with 7. Key variables include
    order details, product name, aisle, and department with respective
    numeric IDs associated with them. Variables include the day of the
    week the activity occurs, the minute in which activity is being
    recorded, and the acceleration count of the 63 y/o male patient’s
    body at each minute. Additionally, the dataset includes variables
    that categorize if the date/minute/count being logged is a weekend
    or weekday.

Aggregate dataset accross minutes to create a total activity variable
for each day, and create a table showing these totals:

  - Trends include:
