Homework 3
================
Connie Zhang
10/09/2019

# Problem 1

``` r
instacart = 
  library(p8105.datasets)
  data("instacart")
```

  - The instacart dataset has 1384617 observations, with 15 variables.
    Key variables include order details, product name, aisle, and
    department with respective numeric IDs associated with them. For
    example, the first order
is

| order\_id | product\_id | add\_to\_cart\_order | reordered | user\_id | eval\_set | order\_number | order\_dow | order\_hour\_of\_day | days\_since\_prior\_order | product\_name    | aisle\_id | department\_id | aisle  | department |
| --------: | ----------: | -------------------: | --------: | -------: | :-------- | ------------: | ---------: | -------------------: | ------------------------: | :--------------- | --------: | -------------: | :----- | :--------- |
|         1 |       49302 |                    1 |         1 |   112108 | train     |             4 |          4 |                   10 |                         9 | Bulgarian Yogurt |       120 |             16 | yogurt | dairy eggs |

It is an order of bulgarian yogurt, by user id 112108, placed on 10am on
4th day of week, Wednesday. The yogurt is in aisle 120, in the dairy
eggs department. He purchased this product 4 times before and his last
order was 9 days ago.

Number of aisles and which aisles the most items are ordered from:

``` r
instacart %>%
  count(aisle_id, name = "n") %>% 
arrange (desc(n))
```

    ## # A tibble: 134 x 2
    ##    aisle_id      n
    ##       <int>  <int>
    ##  1       83 150609
    ##  2       24 150473
    ##  3      123  78493
    ##  4      120  55240
    ##  5       21  41699
    ##  6      115  36617
    ##  7       84  32644
    ##  8      107  31269
    ##  9       91  26240
    ## 10      112  23635
    ## # … with 124 more rows

Plot that shows the number of items ordered in each aisle, limited to
aisles with more than 10000 items ordered:

``` r
instacart %>%
  group_by(aisle) %>%
  summarize(aisle_n = n()) %>%
  filter(aisle_n >10000) %>%
  arrange (desc(aisle_n)) %>%
  ggplot(aes(x = aisle, y = aisle_n, color = aisle)) + ggtitle("Quantity of Items Ordered in Each Aisle") +
  theme(plot.title = element_text(hjust = 0.5))+ geom_point() + 
  labs(       x = "aisle",
              y = "number of items ordered in aisle") + viridis::scale_color_viridis(discrete = TRUE) + theme(legend.position = "none", axis.text.x = element_text(angle=70, hjust=1))
```

![](p8105_hw3_cz2540_files/figure-gfm/unnamed-chunk-3-1.png)<!-- -->

Table that shows three most popular items in the aisles “baking
ingredients”, “dog food care”, and “packaged vegetables fruits”. Include
the number of times each item is ordered in your table:

``` r
table_popular = 
instacart %>%
  filter(aisle %in% c("baking ingredients", "dog food care","packaged vegetables fruits")) %>%
  group_by(aisle, product_name) %>%
  summarize(n = n()) %>%
top_n(3) %>%
arrange(desc(n)) %>%
knitr::kable()
```

    ## Selecting by n

``` r
table_popular
```

| aisle                      | product\_name                                 |    n |
| :------------------------- | :-------------------------------------------- | ---: |
| packaged vegetables fruits | Organic Baby Spinach                          | 9784 |
| packaged vegetables fruits | Organic Raspberries                           | 5546 |
| packaged vegetables fruits | Organic Blueberries                           | 4966 |
| baking ingredients         | Light Brown Sugar                             |  499 |
| baking ingredients         | Pure Baking Soda                              |  387 |
| baking ingredients         | Cane Sugar                                    |  336 |
| dog food care              | Snack Sticks Chicken & Rice Recipe Dog Treats |   30 |
| dog food care              | Organix Chicken & Brown Rice Recipe           |   28 |
| dog food care              | Small Dog Biscuits                            |   26 |

Table that shows the mean hour of the day at which Pink Lady Apples and
Coffee Ice Cream are ordered on each day of the week; format this table
for human readers (i.e. produce a 2 x 7 table):

``` r
table_icecream_apple = instacart %>%
  select(product_name, order_dow, order_hour_of_day) %>%
  group_by(product_name, order_dow) %>%
  summarize(mean_hour = mean(order_hour_of_day)) %>%
  mutate(order_dow = recode(order_dow, "0" = "Sunday", "1" = "Monday", "2" = "Tuesday", "3" = "Wednesday", "4" = "Thursday", "5" = "Friday", "6" = "Saturday")) %>%
  filter(product_name %in% c("Pink Lady Apples", "Coffee Ice Cream")) %>%
  pivot_wider(names_from = "order_dow", values_from = "mean_hour") %>%
knitr::kable(digits = 2)
```

# Problem 2

``` r
data("brfss_smart2010")
cleanbrfss_data = brfss_smart2010 %>%
  janitor::clean_names() %>% 
  separate(locationdesc, into = c("state", "county"), sep=3) %>%
  select(-locationabbr) %>%
  filter(topic == "Overall Health") %>% 
  filter(response %in% c("Poor","Fair","Good","Very good", "Excellent")) %>% 
  mutate(response = factor(response, levels = c("Poor", "Fair","Good", "Very good", "Excellent"))) %>%
  mutate(county = stringr::str_replace(county, "- ", ""))
cleanbrfss_data
```

    ## # A tibble: 10,625 x 23
    ##     year state county class topic question response sample_size data_value
    ##    <int> <chr> <chr>  <chr> <chr> <chr>    <fct>          <int>      <dbl>
    ##  1  2010 "AL " Jeffe… Heal… Over… How is … Excelle…          94       18.9
    ##  2  2010 "AL " Jeffe… Heal… Over… How is … Very go…         148       30  
    ##  3  2010 "AL " Jeffe… Heal… Over… How is … Good             208       33.1
    ##  4  2010 "AL " Jeffe… Heal… Over… How is … Fair             107       12.5
    ##  5  2010 "AL " Jeffe… Heal… Over… How is … Poor              45        5.5
    ##  6  2010 "AL " Mobil… Heal… Over… How is … Excelle…          91       15.6
    ##  7  2010 "AL " Mobil… Heal… Over… How is … Very go…         177       31.3
    ##  8  2010 "AL " Mobil… Heal… Over… How is … Good             224       31.2
    ##  9  2010 "AL " Mobil… Heal… Over… How is … Fair             120       15.5
    ## 10  2010 "AL " Mobil… Heal… Over… How is … Poor              66        6.4
    ## # … with 10,615 more rows, and 14 more variables:
    ## #   confidence_limit_low <dbl>, confidence_limit_high <dbl>,
    ## #   display_order <int>, data_value_unit <chr>, data_value_type <chr>,
    ## #   data_value_footnote_symbol <chr>, data_value_footnote <chr>,
    ## #   data_source <chr>, class_id <chr>, topic_id <chr>, location_id <chr>,
    ## #   question_id <chr>, respid <chr>, geo_location <chr>

States that were observed in 2002 at 7 or more locations:

``` r
cleanbrfss_data %>% 
  filter(year == 2002) %>% 
  group_by(state) %>% 
  summarize(n_location = n_distinct(county)) %>% 
  filter(n_location >= 7) %>%
  arrange(n_location)
```

    ## # A tibble: 6 x 2
    ##   state n_location
    ##   <chr>      <int>
    ## 1 "CT "          7
    ## 2 "FL "          7
    ## 3 "NC "          7
    ## 4 "MA "          8
    ## 5 "NJ "          8
    ## 6 "PA "         10

  - In 2002, Connecticut, Florida, Massachusetts, North Carolina, New
    Jersey, and Pennsylvania were observed at 7 or more locations.

States that were observed in 2010 at 7 or more locations:

``` r
cleanbrfss_data %>% 
  filter(year == 2010) %>% 
  group_by(state) %>% 
  summarize(n_location = n_distinct(county)) %>% 
  filter(n_location >= 7) %>%
  arrange(n_location)
```

    ## # A tibble: 14 x 2
    ##    state n_location
    ##    <chr>      <int>
    ##  1 "CO "          7
    ##  2 "PA "          7
    ##  3 "SC "          7
    ##  4 "OH "          8
    ##  5 "MA "          9
    ##  6 "NY "          9
    ##  7 "NE "         10
    ##  8 "WA "         10
    ##  9 "CA "         12
    ## 10 "MD "         12
    ## 11 "NC "         12
    ## 12 "TX "         16
    ## 13 "NJ "         19
    ## 14 "FL "         41

  - In 2010, Colorado, Pennsylvania, South Carolina, Ohio, Maryland, New
    York, Nebraska, Washington, California, Maryland, North Carolina,
    Texas, New Jersey, and Florida were observed in 7 or more locations.

Dataset limited to Excellent responses, and contains, year, state,
averages of the data\_value across locations within a state:

``` r
brfss_excellent = cleanbrfss_data %>% 
  filter(response == "Excellent") %>%
  group_by(state, year, response) %>% 
  summarize(avg_data = mean(data_value))
```

“Spaghetti” plot of this average value over time within a state (that
is, make a plot showing a line for each state across years):

``` r
brfss_excellent %>%
ggplot(aes(x = year , y = avg_data, color=state)) + geom_line() + 
labs(title = "Average Value Over Time within a State", x = "Year", y = "Average Value") + 
viridis::scale_color_viridis(discrete = TRUE) + theme_minimal() + theme(plot.title = element_text(hjust = 0.5, size = 14), axis.text.x = element_text(angle=70, hjust=1)) + theme(legend.position = "right", axis.text.x = element_text(angle=70, hjust=1))
```

    ## Warning: Removed 3 rows containing missing values (geom_path).

![](p8105_hw3_cz2540_files/figure-gfm/unnamed-chunk-10-1.png)<!-- -->

Two-panel plot showing, for the years 2006, and 2010, distribution of
data\_value for responses (“Poor” to “Excellent”) among locations in NY
State

# Problem 3

``` r
acc_data = 
  read_csv("./data/accel_data.csv") %>%
  janitor::clean_names() 
```

    ## Parsed with column specification:
    ## cols(
    ##   .default = col_double(),
    ##   day = col_character()
    ## )

    ## See spec(...) for full column specifications.

``` r
acc_data
```

    ## # A tibble: 35 x 1,443
    ##     week day_id day   activity_1 activity_2 activity_3 activity_4
    ##    <dbl>  <dbl> <chr>      <dbl>      <dbl>      <dbl>      <dbl>
    ##  1     1      1 Frid…       88.4       82.2       64.4       70.0
    ##  2     1      2 Mond…        1          1          1          1  
    ##  3     1      3 Satu…        1          1          1          1  
    ##  4     1      4 Sund…        1          1          1          1  
    ##  5     1      5 Thur…       47.4       48.8       46.9       35.8
    ##  6     1      6 Tues…       64.8       59.5       73.7       45.7
    ##  7     1      7 Wedn…       71.1      103.        68.5       45.4
    ##  8     2      8 Frid…      675        542       1010        779  
    ##  9     2      9 Mond…      291        335        393        335  
    ## 10     2     10 Satu…       64         11          1          1  
    ## # … with 25 more rows, and 1,436 more variables: activity_5 <dbl>,
    ## #   activity_6 <dbl>, activity_7 <dbl>, activity_8 <dbl>,
    ## #   activity_9 <dbl>, activity_10 <dbl>, activity_11 <dbl>,
    ## #   activity_12 <dbl>, activity_13 <dbl>, activity_14 <dbl>,
    ## #   activity_15 <dbl>, activity_16 <dbl>, activity_17 <dbl>,
    ## #   activity_18 <dbl>, activity_19 <dbl>, activity_20 <dbl>,
    ## #   activity_21 <dbl>, activity_22 <dbl>, activity_23 <dbl>,
    ## #   activity_24 <dbl>, activity_25 <dbl>, activity_26 <dbl>,
    ## #   activity_27 <dbl>, activity_28 <dbl>, activity_29 <dbl>,
    ## #   activity_30 <dbl>, activity_31 <dbl>, activity_32 <dbl>,
    ## #   activity_33 <dbl>, activity_34 <dbl>, activity_35 <dbl>,
    ## #   activity_36 <dbl>, activity_37 <dbl>, activity_38 <dbl>,
    ## #   activity_39 <dbl>, activity_40 <dbl>, activity_41 <dbl>,
    ## #   activity_42 <dbl>, activity_43 <dbl>, activity_44 <dbl>,
    ## #   activity_45 <dbl>, activity_46 <dbl>, activity_47 <dbl>,
    ## #   activity_48 <dbl>, activity_49 <dbl>, activity_50 <dbl>,
    ## #   activity_51 <dbl>, activity_52 <dbl>, activity_53 <dbl>,
    ## #   activity_54 <dbl>, activity_55 <dbl>, activity_56 <dbl>,
    ## #   activity_57 <dbl>, activity_58 <dbl>, activity_59 <dbl>,
    ## #   activity_60 <dbl>, activity_61 <dbl>, activity_62 <dbl>,
    ## #   activity_63 <dbl>, activity_64 <dbl>, activity_65 <dbl>,
    ## #   activity_66 <dbl>, activity_67 <dbl>, activity_68 <dbl>,
    ## #   activity_69 <dbl>, activity_70 <dbl>, activity_71 <dbl>,
    ## #   activity_72 <dbl>, activity_73 <dbl>, activity_74 <dbl>,
    ## #   activity_75 <dbl>, activity_76 <dbl>, activity_77 <dbl>,
    ## #   activity_78 <dbl>, activity_79 <dbl>, activity_80 <dbl>,
    ## #   activity_81 <dbl>, activity_82 <dbl>, activity_83 <dbl>,
    ## #   activity_84 <dbl>, activity_85 <dbl>, activity_86 <dbl>,
    ## #   activity_87 <dbl>, activity_88 <dbl>, activity_89 <dbl>,
    ## #   activity_90 <dbl>, activity_91 <dbl>, activity_92 <dbl>,
    ## #   activity_93 <dbl>, activity_94 <dbl>, activity_95 <dbl>,
    ## #   activity_96 <dbl>, activity_97 <dbl>, activity_98 <dbl>,
    ## #   activity_99 <dbl>, activity_100 <dbl>, activity_101 <dbl>,
    ## #   activity_102 <dbl>, activity_103 <dbl>, activity_104 <dbl>, …
